
# 基于关键帧的轻量化行为识别方法研究
> [论文链接](https://kns.cnki.net/KCMS/detail/11.2179.TH.20200821.1657.038.html)

## 0 引言
- 行为视频由连续的图像序列组成，对其空间域表观特征和时间域运动特征的高效利用可以显著提升行为识别效果。为高效利用行为视频的时间域信息，本文以时域分割网络结构(TSN)为基础，提出了一种结合光流图像、差分图像和并行卷积神经网络的高精度行为识别算法，并对网络结构进行轻量化改进，以进一步提高行为识别算法的速度；同时，为了高效利用视频的表观特征，还构建了一种基于图像特征量的关键帧提取算法，以降低行为视频中运动模糊的干扰，获取高质量的图像序列。

## 1　基于图像特征量的关键帧选取
- 传统的关键帧提取算法主要是利用视频中每一帧的图像特征与相邻帧之间的差异来确定关键帧，常用的图像特征主要有颜色直方图[9]、互信息量[10]和帧间似然比等，难以在场景变化较小的行为视频片段中找出含有关键信息的图像帧。

- 本文提出了一种基于图像特征量的关键帧选取算法，将图像平均边缘强度和图像结构清晰度的乘积表示为图像特征量，以在行为视频片段中找到合适的关键帧.

- 算法具体运算步骤如下:

        1)为待评价图像构造参考图像
        2)提取图像I和Ir的梯度信息
        3)找出图像G和Gr中梯度信息最丰富的N个图像块
        4)计算待评价图像的结构清晰度(Structure Sharpness, SS)，如式(2)所示。图像结构清晰度是一种评价图像质量的关键指标，其与人类视觉感受能够较好的对应，一般模糊的图像结构清晰度也较低
        5)计算待评价图像的平均边缘强度EE，即梯度图像G的平均灰度值
        6)提取视频片段中的关键帧。计算待评价图像的特征量，即图像平均边缘强度和图像结构清晰度的乘积，并选取图像特征量的局部最大值为关键帧

## 2　行为识别算法设计

- 时域分割网络(TSN)是一种基于长时间视频建模的行为识别算法，该算法结合了稀疏时间采样策略和视频级监督方法，可以充分利用整个视频的图像信息，在获取整个视频内行为的长时间信息的同时又保证了识别效率
- 本文在TSN算法基础上进行改进，提出使用并行的光流图像和差分图像代替堆叠光流序列.

        1)将行为视频按均匀时间间隔分为3个视频片段{S1,S2,S3}{S1,S2,S3}。
        2)采用基于图像特征量的方法从相应视频片段中选取关键帧。本文利用轻量化卷积神经网络EfficientNet-B3作为空间域的表观模型(Appearance ConvNet)获得该帧的行为分类得分。EfficientNet-B3可以在模型准确度、尺寸和推理速度之间实现很好的平衡。
        3)稀疏采样视频片段的t个非关键帧，并利用基于卷积神经网络光流计算方法TVNet[14]提取关键帧与非关键帧之间的光流信息，利用帧间差分法计算关键帧与非关键帧之间的差分信息；然后将光流图像和差分图像输入时间域的运动模型(Motion ConvNet)，并平均融合上一个关键帧的分类得分，计算该帧的行为分类得分。
        4)迭代步骤2)和步骤3)直到遍历完所有视频片段。
        5)将每个视频片段的输出结果进行加权平均融合。
        6)将融合得到的分类得分输入Softmax分类器得到视频的行为类别概率。


## 3　实验与分析
### 数据集
- 选取目前行为识别中广泛使用的UCF101数据集和HMDB51数据集

### 实验设置
- 本文测试平台配置如下：计算机配置为Intel Xeon CPU E5-2678v3@2.50GHz，NVIDIA GeForce 2070 GPU双卡，操作系统为Ubuntu 16.04
- 程序基于Pytorch深度学习框架设计实现，CUDA版本为10.0。选用数据集为UCF101和HMDB51，其中训练集70%，测试集30%
- 数据集批次大小均为16，表观模型和运动模型分别采用在ImageNet图像库上预训练的EfficientNet-B3和EfficientNet-B0模型进行初始化, 网络训练过程中采用自适应矩估计算法(Adaptive Moment Estimation, Adam)微调预训练模型

- 对原始数据依次采取了如下三种数据增强方法，以增加训练样本的多样性

        角点裁剪
        尺度抖动
        色彩抖动

### 关键帧选取
- 在对视频进行行为识别时，视频子序列需要融合关键帧与非关键帧的分类得分。对于视频关键帧数量的选定，如果选取的关键帧过多，可能会导致丢失部分运动信息并增加计算资源开销；否则可能限制网络对表观特征的学习，严重影响模型的表达能力

- 实验结果表明选择合适的关键帧比例能够提高行为识别准确率，因此在本文实验中将关键帧比例设置为1/12


### TVNet光流
- 基于双流卷积神经网络的行为识别算法需要事先计算视频序列之间的光流图，传统光流提取方法通常占用大量的存储空间和计算资源
- 为了实现对网络进行端到端的训练，本文采用了基于卷积神经网络光流提取方法TVNet，其通过将传统方法TV-L1的迭代优化过程展开为神经网络构建而成, 因此与同样基于卷积神经网络的FlowNet2.0[17]相比不需要额外的预训练过程；同时与传统的光流计算方法相比，其具有更低的光流预测误差和更快的计算速度
- 实验结果表明，本文采用的基于卷积神经网络的光流计算方法可以进行端到端的训练，并有效降低光流计算误差，有利于提高整个时域网络的行为识别准确率

### 改进TSN算法
- 为了缩短TSN算法建模以及识别的时间，本文的算法采用了更加高效的特征提取网络
- 实验结果如表2所示，本文所采用的轻量化网络模型，与基于三维卷积神经网络的Res3D相比，参数量减少了2.8倍，浮点运算量减少了10.7倍；与基于双流方法的时域分割网络（TSN）相比，参数量减少了2.8倍，浮点运算量减少了1.9倍；与基于时空特征融合的时空乘法器网络（ST-Mult）相比，参数量降低了2.1倍，浮点运算量减少了2.1倍。

- 同时由于光流不包含空间特征，在单独使用光流图像训练运动模型时得到的识别效果欠佳。但是光流提供的运动特征信息与静态RGB图像所提供的表观特征信息可以互相补充，RGB图像和光流图像的分类得分融合后识别准确率提高到了65.4％，可以显着提升行为识别效果；RGB图像和差分图像融合后识别准确率提高到61.7％，表明差分图像相较于光流图像捕获运动信息的能力较差，但其计算成本相对光流更低

- 本文算法与其它经典算法进行了比较分析。由实验对比结果可知，与传统的人工特征设计方法（如IDT）相比，深度卷积神经网络可以很好地学习视频中的一些高级语义特征信息，从而极大的提高行为识别准确率；
- 3D卷积网络可以捕捉时间维度和空间维度的特征信息，且一次处理多帧图像，加快了推理速度，但是3D卷积训练时的计算开销较大，并且识别准确率比利用光流图像的双流网络方法略低（如ECO、C3D）
- 在深度卷积神经网络中，对表观、运动信息进行建模，引入双流卷积神经网络分别提取空域和时域信息的方法能够更好地提高视频行为识别的准确率（如TSN、RepFlow）
- 在UCF101数据集上，原始TSN算法的平均准确率为94.0%，而本文算法最终达到94.7%；在HMDB51数据集上，原始TSN算法的平均准确率为68.5%，本文算法得到了69.3%的平均准确率
- 在速度方面，最终算法速度为79.5FPS，相比原始TSN算法的54.7FPS提升了近45.3%。

## 4 结论
- 本文针对目前双流算法存在信息冗余和计算复杂度高的问题，构建了一种包含表观信息流和运动信息流的改进时域分割网络，将关键帧RGB图像、非关键帧光流图像和差分图像并行地输入特征提取网络计算分类得分，最后将关键帧与非关键帧的行为类别得分进行平均融合后输入SoftMax层得到视频类别概率。
- 实验结果表明，本文的算法在UCF101数据集的识别准确率为94.7%，在HMDB51数据集的识别准确率为69.3%，具有较好的行为识别效果，同时算法推理速度达到79.5FPS，相比原始TSN算法提升了45.3%，能够高效利用视频的表观信息和运动信息。

